{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzulBarr/introduccion-a-las-bases-de-datos/blob/main/5_2_Spark_y_SQL_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyyri890fhDe"
      },
      "source": [
        "# PySpark + SQL\n",
        "\n",
        "PySpark es la interfaz de Python para Apache Spark. Su uso principal es trabajar con grandes volúmenes de datos y crear pipelines de procesamiento.\n",
        "\n",
        "Sin embargo, no es necesario trabajar con big data para aprovechar PySpark. SparkSQL es una excelente herramienta para realizar análisis de datos de forma eficiente. En muchos casos, Pandas puede volverse lento y uno termina escribiendo mucho código para limpiar y transformar datos, mientras que en SQL las mismas operaciones suelen necesitar menos líneas y ser más expresivas. ¡Vamos a comenzar!\n",
        "\n",
        "Más información aquí:\n",
        "http://spark.apache.org/docs/latest/api/python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9yXqV3LigUA"
      },
      "source": [
        "# 1. Instalando PySpark en Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCYXM40ZUMYx"
      },
      "outputs": [],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"5.1 Spark y SQL\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5X-X-5RVCTP"
      },
      "outputs": [],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHvKMqLQ4ezk"
      },
      "source": [
        "# 2. Lectura de datos\n",
        "\n",
        "Utilizamos base publica de datos del COVID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzvNxiQSixRU"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "path = \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv\"\n",
        "req = requests.get(path)\n",
        "url_content = req.content\n",
        "\n",
        "csv_file_name = 'owid-covid-data.csv'\n",
        "csv_file = open(csv_file_name, 'wb')\n",
        "\n",
        "csv_file.write(url_content)\n",
        "csv_file.close()\n",
        "\n",
        "df = spark.read.csv('/content/'+csv_file_name, header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYRUC46L_8zX"
      },
      "source": [
        "#3. PySpark DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-PgzP3IjZsV"
      },
      "outputs": [],
      "source": [
        "# Revisando el schema del dataframe\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KKBv0ZCFbP5"
      },
      "outputs": [],
      "source": [
        "# Conversion date a columna\n",
        "df.select(F.to_date(df.date).alias('date'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2ylA4B2kfd2"
      },
      "outputs": [],
      "source": [
        "#Summary estadisticas\n",
        "df.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRX6qF_dEp9l"
      },
      "outputs": [],
      "source": [
        "#Filtrado de DataFrame.\n",
        "#Pais ARGENTINA ordenados por fecha desc.\n",
        "df.filter(df.location == \"Argentina\").orderBy(F.desc(\"date\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzGaFJ3QEG19"
      },
      "outputs": [],
      "source": [
        "#Agrupamos por location y como funcion de agrupacion sumamos los nuevos casos.\n",
        "df.groupBy(\"location\").sum(\"new_cases\").orderBy(F.desc(\"sum(new_cases)\")).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt6e41cFEFSR"
      },
      "source": [
        "# 4. Spark SQL\n",
        "\n",
        "El módulo SQL resulta muy accesible para interactuar con los datos mientras seguimos usando Spark. Hay menos cosas nuevas que aprender, ya que básicamente utiliza la misma sintaxis SQL con la que probablemente ya estés familiarizado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBpoPIGDrb-c"
      },
      "outputs": [],
      "source": [
        "#Creamos una tabla a partir del data frame\n",
        "df.createOrReplaceTempView(\"covid_data\") # tabla temporal\n",
        "# df.saveAsTable(\"covid_data\") # opcion de salvar la tabla\n",
        "# df.write.mode(\"overwrite\").saveAsTable(\"covid_data\") # Save as table and overwrite table if exits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFcoi5l7kyLq"
      },
      "outputs": [],
      "source": [
        "\n",
        "df2 = spark.sql(\"SELECT * from covid_data\")\n",
        "df2.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe1-rqypYiHg"
      },
      "outputs": [],
      "source": [
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teHD2Up4k4Cd"
      },
      "outputs": [],
      "source": [
        "groupDF = spark.sql(\"SELECT location, count(*) from covid_data group by location order by count(*)\")\n",
        "groupDF.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ks0prWsZL-E"
      },
      "source": [
        "### N: Obtener el total de casos confirmados de COVID-19 por país hasta la fecha más reciente disponible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_imklXKZCOc"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "SELECT location AS pais, sum(total_cases) AS total_casos_confirmados\n",
        "FROM covid_data\n",
        "GROUP BY location\n",
        "ORDER BY total_casos_confirmados DESC\n",
        "\"\"\"\n",
        "\n",
        "result = spark.sql(query)\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04QKs_mKZXwG"
      },
      "source": [
        "### O: Consultar el número de muertes en una fecha específica (2022-01-01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lAMfaTaZBHX"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "SELECT date, sum(total_deaths) AS num_muertes\n",
        "FROM covid_data\n",
        "WHERE date = '2022-01-01'\n",
        "GROUP BY date\n",
        "\"\"\"\n",
        "\n",
        "result = spark.sql(query)\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRnt7MJNZjfn"
      },
      "source": [
        "### P: Obtener la evolución diaria de los casos en un país específico ( \"Argentina\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgp3RJF8ZnOd"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "SELECT location AS pais, date, new_cases\n",
        "FROM covid_data\n",
        "WHERE iso_code = 'ARG'\n",
        "ORDER BY date DESC\n",
        "\"\"\"\n",
        "\n",
        "result = spark.sql(query)\n",
        "result.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7LbbQGrZ2d-"
      },
      "source": [
        "### Q: Calcular porcentaje de la población vacunada por país."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "SELECT location AS pais, max(people_vaccinated_per_hundred) AS porcentaje_personas_vacunadas\n",
        "FROM covid_data\n",
        "WHERE people_vaccinated_per_hundred IS NOT NULL\n",
        "GROUP BY location\n",
        "ORDER BY porcentaje_personas_vacunadas\n",
        "\"\"\"\n",
        "\n",
        "result = spark.sql(query)\n",
        "result.show()"
      ],
      "metadata": {
        "id": "S_3Gy69Rl7gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwOAIyKJaE2V"
      },
      "source": [
        "### R: Calcular el total de casos y muertes en el ultimo mes disponible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Caa91NhuyX3"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "SELECT location AS pais, SUM(new_cases) AS total_casos_mes, SUM(new_deaths) AS total_muertes_mes\n",
        "FROM covid_data\n",
        "WHERE new_cases IS NOT NULL AND new_deaths IS NOT NULL AND month(date) >= (SELECT DISTINCT max(month(date))\n",
        "                                                                          FROM covid_data\n",
        "                                                                          WHERE year(date) >= (SELECT DISTINCT max(year(date))\n",
        "                                                                          FROM covid_data)\n",
        "                                                                          )\n",
        "                                                        AND year(date) >= (SELECT DISTINCT max(year(date))\n",
        "                                                                          FROM covid_data)\n",
        "GROUP BY location\n",
        "ORDER BY total_casos_mes DESC, total_muertes_mes DESC\n",
        "\"\"\"\n",
        "\n",
        "result = spark.sql(query)\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyV5UklAaLd5"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "SELECT location AS pais, SUM(new_cases) AS total_casos_mes, SUM(new_deaths) AS total_muertes_mes\n",
        "FROM covid_data\n",
        "WHERE new_cases IS NOT NULL AND new_deaths IS NOT NULL AND month(date) >= (SELECT DISTINCT max(month(date))\n",
        "                                                                          FROM covid_data\n",
        "                                                                          WHERE year(date) >= (SELECT DISTINCT max(year(date))\n",
        "                                                                          FROM covid_data)\n",
        "                                                                          )\n",
        "GROUP BY location\n",
        "ORDER BY total_casos_mes DESC, total_muertes_mes DESC\n",
        "\"\"\"\n",
        "\n",
        "result = spark.sql(query)\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oDE8fe_-tq4I"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}